# generated by generate-example-output.sh
---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: requester-llm-d-modelservice
  labels:
    helm.sh/chart: llm-d-modelservice-v0.4.7
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/decode-requester-replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: requester-llm-d-modelservice-decode
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dp-app
  template:
    metadata:
      labels:
        app: dp-app
      annotations:
        dual-pods.llm-d.ai/admin-port: "8081"
        dual-pods.llm-d.ai/accelerators: "GPU-0"
        dual-pods.llm-d.ai/inference-server-config: inference-server-config-example
    spec:
      containers:
        - name: inference-server
          image: ghcr.io/llm-d-incubation/llm-d-fast-model-actuation/requester:latest
          imagePullPolicy: Always
          command:
          - /app/requester
          - --node=$(NODE_NAME)
          - --pod-uid=$(POD_UID)
          - --namespace=$(NAMESPACE)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_UID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          ports:
          - name: probes
            containerPort: 8080
          - name: spi
            containerPort: 8081
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 2
            periodSeconds: 5
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: 1
              memory: 250Mi
---
# Source: llm-d-modelservice/templates/prefill-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: requester-llm-d-modelservice-prefill
  labels:
    helm.sh/chart: llm-d-modelservice-v0.4.7
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/inference-serving: "true"
      llm-d.ai/model: facebook-opt-125m
      llm-d.ai/role: prefill
  template:
    metadata:
      labels:
        llm-d.ai/inference-serving: "true"
        llm-d.ai/model: facebook-opt-125m
        llm-d.ai/role: prefill
    spec:

      serviceAccountName: requester-llm-d-modelservice

      volumes:
        - emptyDir: {}
          name: metrics-volume

        - name: model-storage
          emptyDir:
            sizeLimit: 20Gi


      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d-cuda:latest

          command: ["vllm", "serve"]
          args:
            - "facebook/opt-125m"
            - --port
            - "8000"
            - --served-model-name
            - "facebook/opt-125m"


            - --enforce-eager
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
          env:
          - name: CUDA_VISIBLE_DEVICES
            value: "0"
          - name: UCX_TLS
            value: cuda_ipc,cuda_copy,tcp
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5600"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "1"
          - name: DP_SIZE_LOCAL
            value: "1"

          - name: HF_HOME
            value: /model-cache

          ports:
          - containerPort: 8000
            protocol: TCP
          - containerPort: 5600
            protocol: TCP
          resources:
            limits:
              cpu: "16"
              memory: 16Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: "16"
              memory: 16Gi
              nvidia.com/gpu: "1"

          volumeMounts:
            - name: model-storage
              mountPath: /model-cache
---
# Source: llm-d-modelservice/templates/decode-requester-replicaset.yaml
apiVersion: fma.llm-d.ai/v1alpha1
kind: InferenceServerConfig
metadata:
  name: inference-server-config-example
  annotations:
    description: Example InferenceServerConfig
spec:
  launcherConfigName: launcher-config-example
  modelServerConfig:
    labels:
      model-reg: "opt-125m"
      model-repo: "facebook"
    port: 8005
    initContainers:
      - name: routing-proxy
        args:
          - --port=8000
          - --vllm-port=8200
          - --connector=nixlv2
          - --zap-encoder=json
          - --zap-log-level=debug
          - --secure-proxy=false
        image: ghcr.io/llm-d/llm-d-routing-sidecar:latest
        imagePullPolicy: Always
        ports:
          - containerPort: 8000
        resources: {}
        restartPolicy: Always
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
    containers:
      - name: vllm
        image: ghcr.io/llm-d/llm-d-cuda:latest

        command: ["vllm", "serve"]
        args:
          - "facebook/opt-125m"
          - --port
          - "8200"
          - --served-model-name
          - "facebook/opt-125m"


          - --enforce-eager
          - --kv-transfer-config
          - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: UCX_TLS
          value: cuda_ipc,cuda_copy,tcp
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5600"
        - name: VLLM_LOGGING_LEVEL
          value: DEBUG
        - name: DP_SIZE
          value: "1"
        - name: TP_SIZE
          value: "1"
        - name: DP_SIZE_LOCAL
          value: "1"

        - name: HF_HOME
          value: /model-cache

        ports:
        - containerPort: 8200
          protocol: TCP
        - containerPort: 5600
          protocol: TCP
        resources:
          limits:
            cpu: "16"
            memory: 16Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "16"
            memory: 16Gi
            nvidia.com/gpu: "1"

        volumeMounts:
          - name: model-storage
            mountPath: /model-cache
---
# Source: llm-d-modelservice/templates/decode-requester-replicaset.yaml
apiVersion: fma.llm-d.ai/v1alpha1
kind: LauncherConfig
metadata:
  name: launcher-config-example
spec:
  maxSleepingInstances: 3
  podTemplate:
    spec:
      containers:
      - args:
        - |
          uvicorn launcher:app --host 0.0.0.0 --log-level info --port 8001
        command:
        - /bin/bash
        - -c
        image: ghcr.io/llm-d-incubation/llm-d-fast-model-actuation/launcher:latest
        imagePullPolicy: IfNotPresent
        name: inference-server
