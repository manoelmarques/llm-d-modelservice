# generated by generate-example-output.sh
---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: xpu-pd-llm-d-modelservice
  labels:
    helm.sh/chart: llm-d-modelservice-v0.4.9
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/decode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: xpu-pd-llm-d-modelservice-decode
  labels:
    helm.sh/chart: llm-d-modelservice-v0.4.9
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/inference-serving: "true"
      llm-d.ai/model: microsoft-dialogpt-large
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/inference-serving: "true"
        llm-d.ai/model: microsoft-dialogpt-large
        llm-d.ai/role: decode
    spec:
      initContainers:
        - name: routing-proxy
          args:
            - --port=8000
            - --vllm-port=8200
            - --connector=nixlv2
            - --zap-encoder=json
            - --zap-log-level=debug
            - --secure-proxy=false
          image: ghcr.io/llm-d/llm-d-routing-sidecar:latest
          imagePullPolicy: Always
          env:
          ports:
            - containerPort: 8000
          resources: {}
          restartPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
    
      serviceAccountName: xpu-pd-llm-d-modelservice
      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: accelerator
                  operator: In
                  values:
                    - intel-xpu
      volumes:
        - emptyDir: {}
          name: metrics-volume
      
        - name: model-storage
          emptyDir:
            sizeLimit: 10Gi
        
      
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d-xpu:latest
          imagePullPolicy: Never
          
          command:
            - python3
            - -m
            - vllm.entrypoints.openai.api_server
          args:
            - --model
            - microsoft/DialoGPT-large
            - --enforce-eager
            - --tensor-parallel-size
            - "1"
            - --port
            - "8200"
            - --host
            - 0.0.0.0
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_consumer"}'
          env:
          - name: ZE_AFFINITY_MASK
            value: "0"
          - name: ZE_ENABLE_PCI_ID_DEVICE_ORDER
            value: "1"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5600"
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: TORCH_LLM_ALLREDUCE
            value: "1"
          - name: VLLM_USE_V1
            value: "1"
          - name: CCL_ZE_IPC_EXCHANGE
            value: pidfd
          - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
            value: "1"
          - name: VLLM_WORKER_MULTIPROC_METHOD
            value: spawn
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "1"
          - name: DP_SIZE_LOCAL
            value: "1"
          
          - name: HF_HOME
            value: /model-cache
          
          - name: VLLM_USE_V1
            value: "1"
          - name: TORCH_LLM_ALLREDUCE
            value: "1"
          - name: VLLM_WORKER_MULTIPROC_METHOD
            value: "spawn"
          
          ports:
          - containerPort: 8200
            protocol: TCP
          - containerPort: 5600
            protocol: TCP
          resources:
            limits:
              cpu: "8"
              gpu.intel.com/i915: "1"
              memory: 24Gi
            requests:
              cpu: "4"
              gpu.intel.com/i915: "1"
              memory: 12Gi
          
          volumeMounts:
            - name: model-storage
              mountPath: /model-cache
---
# Source: llm-d-modelservice/templates/prefill-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: xpu-pd-llm-d-modelservice-prefill
  labels:
    helm.sh/chart: llm-d-modelservice-v0.4.9
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/inference-serving: "true"
      llm-d.ai/model: microsoft-dialogpt-large
      llm-d.ai/role: prefill
  template:
    metadata:
      labels:
        llm-d.ai/inference-serving: "true"
        llm-d.ai/model: microsoft-dialogpt-large
        llm-d.ai/role: prefill
    spec:
    
      serviceAccountName: xpu-pd-llm-d-modelservice
      
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: accelerator
                  operator: In
                  values:
                    - intel-xpu
      volumes:
        - emptyDir: {}
          name: metrics-volume
      
        - name: model-storage
          emptyDir:
            sizeLimit: 10Gi
        
      
      containers:
        - name: vllm
          image: ghcr.io/llm-d/llm-d-xpu:latest
          
          command:
            - python3
            - -m
            - vllm.entrypoints.openai.api_server
          args:
            - --model
            - microsoft/DialoGPT-large
            - --enforce-eager
            - --tensor-parallel-size
            - "1"
            - --port
            - "8000"
            - --host
            - 0.0.0.0
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_producer"}'
          env:
          - name: ZE_AFFINITY_MASK
            value: "1"
          - name: ZE_ENABLE_PCI_ID_DEVICE_ORDER
            value: "1"
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5600"
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          - name: TORCH_LLM_ALLREDUCE
            value: "1"
          - name: VLLM_USE_V1
            value: "1"
          - name: CCL_ZE_IPC_EXCHANGE
            value: pidfd
          - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
            value: "1"
          - name: VLLM_WORKER_MULTIPROC_METHOD
            value: spawn
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "1"
          - name: DP_SIZE_LOCAL
            value: "1"
          
          - name: HF_HOME
            value: /model-cache
          
          - name: VLLM_USE_V1
            value: "1"
          - name: TORCH_LLM_ALLREDUCE
            value: "1"
          - name: VLLM_WORKER_MULTIPROC_METHOD
            value: "spawn"
          
          ports:
          - containerPort: 8000
            protocol: TCP
          - containerPort: 5600
            protocol: TCP
          resources:
            limits:
              cpu: "16"
              gpu.intel.com/i915: "1"
              memory: 32Gi
            requests:
              cpu: "8"
              gpu.intel.com/i915: "1"
              memory: 16Gi
          
          volumeMounts:
            - name: model-storage
              mountPath: /model-cache
