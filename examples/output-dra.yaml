# generated by generate-example-output.sh
---
# Source: llm-d-modelservice/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dra-llm-d-modelservice
  labels:
    helm.sh/chart: llm-d-modelservice-v0.4.9
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: llm-d-modelservice/templates/decode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dra-llm-d-modelservice-decode
  labels:
    helm.sh/chart: llm-d-modelservice-v0.4.9
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/inference-serving: "true"
      llm-d.ai/model: random-model
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/inference-serving: "true"
        llm-d.ai/model: random-model
        llm-d.ai/role: decode
    spec:
    
      serviceAccountName: dra-llm-d-modelservice
      
      volumes:
        - emptyDir: {}
          name: metrics-volume
      
        - name: model-storage
          persistentVolumeClaim:
            claimName: model-pvc
            readOnly: true
      resourceClaims:
        - name: intel-gaudi-decode-claim
          resourceClaimTemplateName: intel-gaudi-claim-template-decode
        - name: llm-d-existing-claim-sample
          resourceClaimTemplateName: llm-d-existing-claim-sample
      containers:
        - name: vllm
          image: intel/vllm:0.10.0-xpu
          
          command: ["vllm", "serve"]
          args:
            - meta-llama/Llama-3.1-8B-Instruct
            - --port
            - "8000"
            - --tensor-parallel-size
            - "2"
            - --served-model-name
            - "random/model"
            
            
            - --dtype=float16
            - --block-size=64
            - --max-num-seqs=64
            - --max-model-len=2048
            - --max-num-batched-token=4096
            - --disable-sliding-window
            - --gpu-memory-util=0.9
            - --quantization=fp8
          env:
          - name: OMPI_MCA_btl_vader_single_copy_mechanism
            value: none
          - name: PT_HPU_ENABLE_LAZY_COLLECTIVES
            value: "true"
          - name: VLLM_SKIP_WARMUP
            value: "true"
          - name: DP_SIZE
            value: "1"
          - name: TP_SIZE
            value: "2"
          - name: DP_SIZE_LOCAL
            value: "1"
          
          - name: HF_HUB_CACHE
            value: /model-cache/
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: llm-d-hf-token
                key: HF_TOKEN
          
          resources:
            limits:
              cpu: "4"
              memory: 16Gi
            requests:
              cpu: "2"
              memory: 8Gi
            claims:
              - name: intel-gaudi-decode-claim
              - name: llm-d-existing-claim-sample
          
          volumeMounts:
            - name: model-storage
              mountPath: /model-cache
---
# Source: llm-d-modelservice/templates/resource-claim-template.yaml
apiVersion: resource.k8s.io/v1
kind: ResourceClaimTemplate
metadata:
  name: intel-gaudi-claim-template-decode
  labels:
    helm.sh/chart: llm-d-modelservice-v0.4.9
    app.kubernetes.io/version: "v0.4.0"
    app.kubernetes.io/managed-by: Helm
    llm-d.ai/role: decode
spec:
  spec:
    devices:
      requests:
      - name: intel-gaudi
        exactly:
          deviceClassName: gaudi.intel.com
          count: 2
