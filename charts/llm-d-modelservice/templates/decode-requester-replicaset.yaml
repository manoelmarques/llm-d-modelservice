{{- if and .Values.requester.enable (and .Values.decode.create (not .Values.multinode)) }}
---
apiVersion: fma.llm-d.ai/v1alpha1
kind: LauncherConfig
metadata:
  name: {{ .Values.requester.launcherConfig | required "requester.launcherConfig is required when requester is enabled" }}
spec:
  maxSleepingInstances: {{ .Values.requester.launcherConfigSpec.maxSleepingInstances | default 3 }}
  podTemplate:
    {{- with .Values.requester.launcherConfigSpec.podTemplate.metadata }}
    metadata:
      {{- toYaml . | nindent 6 }}
    {{- end }}
    spec:
      {{- with .Values.requester.launcherConfigSpec.podTemplate.spec }}
      {{- toYaml . | nindent 6 }}
      {{- end }}
---
apiVersion: fma.llm-d.ai/v1alpha1
kind: InferenceServerConfig
metadata:
  name: {{ .Values.requester.inferenceServerConfig }}
spec:
  launcherConfigName: {{ .Values.requester.launcherConfig }}
  modelServerConfig:
    {{- with .Values.requester.modelServerConfig.annotations }}
    annotations:
      {{- toYaml . | nindent 6 }}
    {{- end }}
    {{- with .Values.requester.modelServerConfig.labels }}
    labels:
      {{- toYaml . | nindent 6 }}
    {{- end }}
    {{- with .Values.requester.modelServerConfig.env_vars }}
    env_vars:
      {{- toYaml . | nindent 6 }}
    {{- end }}
    {{- if .Values.requester.modelServerConfig.options }}
    options: {{ .Values.requester.modelServerConfig.options | quote }}
    {{- end }}
    port: {{ .Values.requester.modelServerConfig.port | default 8005 }}
---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: {{ include "llm-d-modelservice.decodeName" . }}
spec:
  replicas: {{ ternary .Values.decode.replicas 1 (hasKey .Values.decode "replicas") }}
  selector:
    matchLabels:
      app: dp-app
  template:
    metadata:
      labels:
        app: dp-app
      annotations:
        dual-pods.llm-d.ai/admin-port: "{{ .Values.requester.adminPort }}"
        dual-pods.llm-d.ai/accelerators: {{ .Values.requester.accelerators | quote }}
        dual-pods.llm-d.ai/inference-server-config: {{ .Values.requester.inferenceServerConfig | required "requester.inferenceServerConfig is required" }}
    spec:
      containers:
        - name: inference-server
          image: {{ .Values.requester.image }}
          imagePullPolicy: Always
          command:
          - /app/requester
          - --logtostderr=false
          - --log_file=/tmp/requester.log
          ports:
          - name: probes
            containerPort: {{ .Values.requester.port.probes }}
          - name: spi
            containerPort: {{ .Values.requester.port.spi }}
          readinessProbe:
            httpGet:
              path: /ready
              port: {{ .Values.requester.port.probes }}
            initialDelaySeconds: {{ .Values.requester.readinessProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.requester.readinessProbe.periodSeconds }}
          resources:
            limits:
              nvidia.com/gpu: {{ .Values.requester.resources.limits.gpus }}
              cpu: {{ .Values.requester.resources.limits.cpus }}
              memory: {{ .Values.requester.resources.limits.memory }}
{{- end }}
